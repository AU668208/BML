{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a915a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1e97a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86321562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sti til filen. I denne Notebook er alt data samlet i en mappe og herefter splittet.\n",
    "path = 'C:/Users/kvjkv/OneDrive - Aarhus Universitet/7. Semester/BML - Biomedical MachineLearning/prostate158/'\n",
    "File = open(path+'train.csv')\n",
    "type(File)\n",
    "csvreader = csv.reader(File)\n",
    "header = []\n",
    "header = next(csvreader)\n",
    "rows = []\n",
    "with open(path+'train.csv') as File:\n",
    "    csvreader = csv.reader(File)\n",
    "    header = next(csvreader)\n",
    "    for row in csvreader:\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968aaef",
   "metadata": {},
   "source": [
    "Opsætning inden indlæsning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1929b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_function(path1):\n",
    "    img = nib.load(path1).get_fdata()\n",
    "    desired_depth = 24\n",
    "    desired_width = 270\n",
    "    desired_height = 270\n",
    "\n",
    "    actual_depth = img.shape[-1]\n",
    "    actual_width = img.shape[0]\n",
    "    actual_height = img.shape[1]\n",
    "\n",
    "    depthfactor = 1/(actual_depth/desired_depth)\n",
    "    widthfactor = 1/(actual_width/desired_width)\n",
    "    heightfactor = 1/(actual_height/desired_height)\n",
    "\n",
    "    newimg = ndimage.zoom(img, (widthfactor, heightfactor, depthfactor), order=1)\n",
    "    return newimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abffbf4",
   "metadata": {},
   "source": [
    "Indlæsning af træningsdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0d8525",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.empty([159,270,270,24],dtype=float);\n",
    "Y = np.ones(159);\n",
    "pathPatientList = path+'train/';\n",
    "dirs = os.listdir(pathPatientList)\n",
    "val = rows[0][0];\n",
    "i = 0;\n",
    "\n",
    "for idx, x in enumerate(dirs):\n",
    "    if int(x) is int(val):\n",
    "        X[i] = zoom_function(path+rows[i][1])\n",
    "        \n",
    "        if os.path.isfile(pathPatientList + x + '/empty.nii.gz'):\n",
    "            Y[i] = 0    \n",
    "        \n",
    "        if i < len(rows)-1:\n",
    "            i+=1;\n",
    "            val = rows[i][0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4c2b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 270, 270, 24)\n",
      "(159,)\n"
     ]
    }
   ],
   "source": [
    "X_norm = X.astype('float32') / 4096\n",
    "\n",
    "print(X_norm.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d8597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (127, 270, 270, 24)\n",
      "Training label shape: (127,)\n",
      "<class 'numpy.ndarray'>\n",
      "Validation set shape: (16, 270, 270, 24)\n",
      "Validation label shape: (16,)\n",
      "<class 'numpy.ndarray'>\n",
      "Test set shape: (16, 270, 270, 24)\n",
      "Test label shape: (16,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "## Split af data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_norm, Y, test_size=0.2, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Training label shape:\", y_train.shape)\n",
    "print(type(y_train))\n",
    "print(\"Validation set shape:\", x_val.shape)\n",
    "print(\"Validation label shape:\", y_val.shape)\n",
    "print(type(y_val))\n",
    "print(\"Test set shape:\", x_test.shape)\n",
    "print(\"Test label shape:\", y_test.shape)\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e9974af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 534s 133s/step - loss: 4.6725 - accuracy: 0.4016 - val_loss: 0.6595 - val_accuracy: 0.8750\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6799 - accuracy: 0.5625\n",
      "Test accuracy: 56.25%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(270, 270, 24,1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(158, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for binary classification (sigmoid activation for 0 to 1 range)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val))\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test)[1]\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff13905",
   "metadata": {},
   "source": [
    "As seen below, by adding a simple custom Stochastic gradient descent we have improved the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e926c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 238s 45s/step - loss: 0.0000e+00 - accuracy: 0.3858 - val_loss: 0.0000e+00 - val_accuracy: 0.1875\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0000e+00 - accuracy: 0.3750\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(270, 270, 24,1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(158, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for binary classification (sigmoid activation for 0 to 1 range)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Trying to this time use a custom optimizer, still a SGD, however now we can tweak the hyper parameters.\n",
    "custom_sgd = SGD(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=custom_sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val))\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test)[1]\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed146925",
   "metadata": {},
   "source": [
    "Now we try and tweak the hyperparameters of the custom sgd, to see what happens when the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e679092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 174s 41s/step - loss: 0.0000e+00 - accuracy: 0.3622 - val_loss: 0.0000e+00 - val_accuracy: 0.1875\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0000e+00 - accuracy: 0.3750\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(270, 270, 24,1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(158, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for binary classification (sigmoid activation for 0 to 1 range)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Trying to this time use a custom optimizer, still a SGD, however now we can tweak the hyper parameters.\n",
    "custom_sgd = SGD(learning_rate=0.05)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=custom_sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val))\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test)[1]\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c755fec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 162s 39s/step - loss: 0.0000e+00 - accuracy: 0.3543 - val_loss: 0.0000e+00 - val_accuracy: 0.1875\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 0.3750\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(270, 270, 24,1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(158, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for binary classification (sigmoid activation for 0 to 1 range)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Trying to this time use a custom optimizer, still a SGD, however now we can tweak the hyper parameters.\n",
    "custom_sgd = SGD(learning_rate=0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=custom_sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val))\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test)[1]\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441b48",
   "metadata": {},
   "source": [
    "So a higher learning rate seems to help. However we can also try making the learning rate gradually reduce every iteration to avoid overshooting as much when nearing a minimum, this is done by using the exponentialDecay function from keras, which does exactly what the name implies and decreases the initial learning rate exponentially over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3948c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.3937 WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000001368D5D7760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "4/4 [==============================] - 184s 42s/step - loss: 0.0000e+00 - accuracy: 0.3937 - val_loss: nan - val_accuracy: 0.1875\n",
      "1/1 [==============================] - 3s 3s/step - loss: nan - accuracy: 0.3750\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(270, 270, 24,1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(158, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for binary classification (sigmoid activation for 0 to 1 range)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Still using a custom SGD however now a custom learning rate from keras, the exponentialdecay.\n",
    "initial_learning_rate = 0.3\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate,decay_steps, decay_rate)\n",
    "\n",
    "custom_sgd = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=custom_sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val))\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test)[1]\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
